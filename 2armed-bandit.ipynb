{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Optional, Tuple\n",
    "import numpy as np\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sweetbean import Block, Experiment\n",
    "from sweetbean.stimulus import Bandit, Text\n",
    "from sweetbean.variable import (\n",
    "    DataVariable,\n",
    "    FunctionVariable,\n",
    "    SharedVariable,\n",
    "    SideEffect,\n",
    "    TimelineVariable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome message\n",
    "instruction_welcome = Text(\n",
    "    text='Welcome to our decision-making experiment. In this experiment, you will make decisions between two options. \\\n",
    "    Press the SPACE key to continue.',\n",
    "    choices=[' ']\n",
    ")\n",
    "\n",
    "# Explanation of the task\n",
    "instruction_task = Text(\n",
    "    text=json.dumps(\"\"\"\n",
    "    <div class=\"slotmachine\" style=\"position: absolute; top:1vh; left:0 vw; width: 20vw; height: 20vh; border-color:orange\n",
    "        \">\n",
    "        </div>\n",
    "        <div class=\"slotmachine\" style=\"position: absolute; top:1vh; right:0 vw; left:40vw; width: 20vw; height: 20vh; border-color:blue\n",
    "        \">\n",
    "        </div>\n",
    "    \"\"\")+ 'In each trial, you will see two slot machines (bandits) on the screen. <b> Click on a bandit </b> to make your choice\\\n",
    "    Press SPACE to continue.',\n",
    "    choices=[' ']\n",
    ")\n",
    "\n",
    "# Explanation of feedback/reward system\n",
    "instruction_feedback = Text(\n",
    "    text='After selecting a bandit, you will receive feedback showing whether you won reward (1 point) or not (0 point). One bandit tends to give rewards more often than the other. However, this may change during the experiment.\\\n",
    "    Press SPACE to continue.',\n",
    "    choices=[' ']\n",
    ")\n",
    "\n",
    "# End instructions\n",
    "instruction_end = Text(\n",
    "    text='Your goal is to earn as many points as possible. Pay attention to feedback, as the probabilities of winning for each bandit may change over time. \\\n",
    "    You are now ready to begin. Good luck! Press Q to start the task.',\n",
    "    choices=['q', ' ']\n",
    ")\n",
    "\n",
    "# Create a list of instruction \n",
    "# for the instruction block\n",
    "instruction_list = [\n",
    "    instruction_welcome,\n",
    "    instruction_task,\n",
    "    instruction_feedback,\n",
    "    instruction_end\n",
    "]\n",
    "\n",
    "# Create the instruction block\n",
    "instruction_block = Block(instruction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditsGeneral:\n",
    "    \"\"\"\n",
    "    A generalized form of the multi-armed bandit which can enable a drifting paradigm, \n",
    "    reversal learning, reward/no-reward and reward/penalty schedules, and arm-correlations.\n",
    "    \n",
    "    Features:\n",
    "    - Drift: Reward probabilities change gradually over time\n",
    "    - Reversals: Sudden swaps in reward probabilities (controlled by hazard rate)\n",
    "    - Correlated arms: Reward probabilities can move together\n",
    "    - Flexible reward schedules: Binary rewards (0/1) or penalty schedules (-1/+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_arms: int = 2,\n",
    "        init_reward_prob: Optional[Iterable[float]] = None,\n",
    "        drift_rate: float = 0.0,\n",
    "        hazard_rate: float = 0.0,\n",
    "        reward_prob_correlation: float = 0.0,\n",
    "        reward_schedule: str = \"binary\",  # \"binary\" (0/1) or \"penalty\" (-1/+1)\n",
    "        bounds: Tuple[float, float] = (0.0, 1.0),\n",
    "        seed: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_arms: Number of arms\n",
    "            init_reward_prob: Initial reward probabilities for each arm\n",
    "            drift_rate: Rate of Gaussian random walk drift (std dev per step)\n",
    "            hazard_rate: Probability of reversal on each step\n",
    "            reward_prob_correlation: Correlation between arm drifts (-1 to 1)\n",
    "            reward_schedule: \"binary\" for 0/1 rewards, \"penalty\" for -1/+1 rewards\n",
    "            bounds: Min and max values for reward probabilities\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.drift_rate = drift_rate\n",
    "        self.hazard_rate = hazard_rate\n",
    "        self.reward_prob_correlation = reward_prob_correlation\n",
    "        self.reward_schedule = reward_schedule\n",
    "        self.bounds = bounds\n",
    "        \n",
    "        # Random number generator\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        \n",
    "        # Initialize reward probabilities\n",
    "        if init_reward_prob is None:\n",
    "            init_reward_prob = self.rng.uniform(bounds[0], bounds[1], n_arms)\n",
    "        else:\n",
    "            init_reward_prob = np.array(init_reward_prob)\n",
    "            if len(init_reward_prob) != n_arms:\n",
    "                raise ValueError(f\"init_reward_prob must have length {n_arms}\")\n",
    "        \n",
    "        self.init_reward_prob = np.array(init_reward_prob)\n",
    "        self.reward_prob = self.init_reward_prob.copy()\n",
    "        \n",
    "        # Tracking\n",
    "        self.t = 0\n",
    "        self.history = {\n",
    "            'choices': [],\n",
    "            'rewards': [],\n",
    "            'reward_probs': [self.reward_prob.copy()],\n",
    "            'reversals': []\n",
    "        }\n",
    "        \n",
    "        # For correlated drift\n",
    "        if self.reward_prob_correlation != 0 and self.n_arms == 2:\n",
    "            # Construct covariance matrix for bivariate normal\n",
    "            self.drift_cov = np.array([\n",
    "                [1, self.reward_prob_correlation],\n",
    "                [self.reward_prob_correlation, 1]\n",
    "            ]) * (self.drift_rate ** 2)\n",
    "        else:\n",
    "            self.drift_cov = None\n",
    "    \n",
    "    def step(self, choice: int) -> Tuple[float, dict]:\n",
    "        \"\"\"\n",
    "        Execute one step: apply drift/reversals, then generate reward for chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            choice: Index of chosen arm (0 to n_arms-1)\n",
    "            \n",
    "        Returns:\n",
    "            reward: The reward received\n",
    "            info: Dictionary with additional information\n",
    "        \"\"\"\n",
    "        if choice < 0 or choice >= self.n_arms:\n",
    "            raise ValueError(f\"Choice must be between 0 and {self.n_arms-1}\")\n",
    "        \n",
    "        # Apply drift and reversals BEFORE reward is generated\n",
    "        reversal_occurred = self._apply_dynamics()\n",
    "        \n",
    "        # Generate reward based on current probabilities\n",
    "        reward = self._generate_reward(choice)\n",
    "        \n",
    "        # Update history\n",
    "        self.history['choices'].append(choice)\n",
    "        self.history['rewards'].append(reward)\n",
    "        self.history['reward_probs'].append(self.reward_prob.copy())\n",
    "        self.history['reversals'].append(reversal_occurred)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        info = {\n",
    "            'reward_prob': self.reward_prob[choice],\n",
    "            'all_reward_probs': self.reward_prob.copy(),\n",
    "            'reversal': reversal_occurred,\n",
    "            'timestep': self.t\n",
    "        }\n",
    "        \n",
    "        return reward, info\n",
    "    \n",
    "    def _apply_dynamics(self) -> bool:\n",
    "        \"\"\"Apply drift and check for reversals.\"\"\"\n",
    "        reversal_occurred = False\n",
    "        \n",
    "        # Check for reversal (sudden swap)\n",
    "        if self.hazard_rate > 0 and self.rng.random() < self.hazard_rate:\n",
    "            self._apply_reversal()\n",
    "            reversal_occurred = True\n",
    "        \n",
    "        # Apply gradual drift\n",
    "        if self.drift_rate > 0:\n",
    "            self._apply_drift()\n",
    "        \n",
    "        return reversal_occurred\n",
    "    \n",
    "    def _apply_reversal(self):\n",
    "        \"\"\"Apply a reversal: swap the reward probabilities.\"\"\"\n",
    "        if self.n_arms == 2:\n",
    "            # Simple swap for 2 arms\n",
    "            self.reward_prob = self.reward_prob[::-1]\n",
    "        else:\n",
    "            # For >2 arms, randomly permute\n",
    "            self.reward_prob = self.rng.permutation(self.reward_prob)\n",
    "    \n",
    "    def _apply_drift(self):\n",
    "        \"\"\"Apply Gaussian random walk drift to reward probabilities.\"\"\"\n",
    "        if self.drift_cov is not None and self.n_arms == 2:\n",
    "            # Correlated drift for 2 arms\n",
    "            drift = self.rng.multivariate_normal(np.zeros(2), self.drift_cov)\n",
    "        else:\n",
    "            # Independent drift\n",
    "            drift = self.rng.normal(0, self.drift_rate, self.n_arms)\n",
    "        \n",
    "        # Apply drift and clip to bounds\n",
    "        self.reward_prob = np.clip(\n",
    "            self.reward_prob + drift,\n",
    "            self.bounds[0],\n",
    "            self.bounds[1]\n",
    "        )\n",
    "    \n",
    "    def _generate_reward(self, choice: int) -> float:\n",
    "        \"\"\"Generate reward for chosen arm based on current probability.\"\"\"\n",
    "        # Bernoulli trial\n",
    "        success = self.rng.random() < self.reward_prob[choice]\n",
    "        \n",
    "        if self.reward_schedule == \"binary\":\n",
    "            return 1.0 if success else 0.0\n",
    "        elif self.reward_schedule == \"penalty\":\n",
    "            return 1.0 if success else -1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reward_schedule: {self.reward_schedule}\")\n",
    "    \n",
    "    def new_sess(self):\n",
    "        \"\"\"Reset to initial state for a new session.\"\"\"\n",
    "        self.reward_prob = self.init_reward_prob.copy()\n",
    "        self.t = 0\n",
    "        self.history = {\n",
    "            'choices': [],\n",
    "            'rewards': [],\n",
    "            'reward_probs': [self.reward_prob.copy()],\n",
    "            'reversals': []\n",
    "        }\n",
    "    \n",
    "    def get_optimal_arm(self) -> int:\n",
    "        \"\"\"Return the index of the arm with highest current reward probability.\"\"\"\n",
    "        return int(np.argmax(self.reward_prob))\n",
    "    \n",
    "    def get_history_array(self) -> dict:\n",
    "        \"\"\"Return history as numpy arrays for analysis.\"\"\"\n",
    "        return {\n",
    "            'choices': np.array(self.history['choices']),\n",
    "            'rewards': np.array(self.history['rewards']),\n",
    "            'reward_probs': np.array(self.history['reward_probs']),\n",
    "            'reversals': np.array(self.history['reversals'])\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BanditsGeneral with negatively correlated drift\n",
    "bandit = BanditsGeneral(\n",
    "    n_arms=2,\n",
    "    init_reward_prob=[0.6, 0.4],  # Initial probabilities for Bandit 1 and Bandit 2\n",
    "    drift_rate=0.02,              # Drift rate for Gaussian random walk\n",
    "    reward_schedule=\"binary\",  # Binary reward schedule (0/1)\n",
    "    reward_prob_correlation=-0.8, # Negative correlation between arms\n",
    "    seed=42                       # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Simulate rewards\n",
    "rewards_bandit_1 = []\n",
    "rewards_bandit_2 = []\n",
    "n_trials=10\n",
    "for _ in range(n_trials):\n",
    "    # Simulate a step for Bandit 1\n",
    "    reward_1, _ = bandit.step(0)  # Choose Bandit 1 (index 0)\n",
    "    rewards_bandit_1.append(reward_1)\n",
    "    \n",
    "    # Simulate a step for Bandit 2\n",
    "    reward_2, _ = bandit.step(1)  # Choose Bandit 2 (index 1)\n",
    "    rewards_bandit_2.append(reward_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_bandit_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_bandit_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: {'bandit_1': {'color': 'orange', 'value': 0.0}, 'bandit_2': {'color': 'blue', 'value': 0.0}}\n",
      "Trial 2: {'bandit_1': {'color': 'orange', 'value': 1.0}, 'bandit_2': {'color': 'blue', 'value': 0.0}}\n",
      "Trial 3: {'bandit_1': {'color': 'orange', 'value': 1.0}, 'bandit_2': {'color': 'blue', 'value': 1.0}}\n",
      "Trial 4: {'bandit_1': {'color': 'orange', 'value': 0.0}, 'bandit_2': {'color': 'blue', 'value': 0.0}}\n",
      "Trial 5: {'bandit_1': {'color': 'orange', 'value': 1.0}, 'bandit_2': {'color': 'blue', 'value': 0.0}}\n",
      "Trial 6: {'bandit_1': {'color': 'orange', 'value': 1.0}, 'bandit_2': {'color': 'blue', 'value': 1.0}}\n",
      "Trial 7: {'bandit_1': {'color': 'orange', 'value': 1.0}, 'bandit_2': {'color': 'blue', 'value': 0.0}}\n",
      "Trial 8: {'bandit_1': {'color': 'orange', 'value': 0.0}, 'bandit_2': {'color': 'blue', 'value': 1.0}}\n",
      "Trial 9: {'bandit_1': {'color': 'orange', 'value': 1.0}, 'bandit_2': {'color': 'blue', 'value': 0.0}}\n",
      "Trial 10: {'bandit_1': {'color': 'orange', 'value': 1.0}, 'bandit_2': {'color': 'blue', 'value': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# Define colors for bandits\n",
    "bandit_colors = {\n",
    "    \"bandit_1\": \"orange\",\n",
    "    \"bandit_2\": \"blue\"\n",
    "}\n",
    "# format rewards_bandit_1 and rewards_bandit_2 into timeline structure\n",
    "timeline = []\n",
    "for r1, r2 in zip(rewards_bandit_1, rewards_bandit_2):\n",
    "    timeline.append({\n",
    "        \"bandit_1\": {\"color\": bandit_colors[\"bandit_1\"], \"value\": r1},\n",
    "        \"bandit_2\": {\"color\": bandit_colors[\"bandit_2\"], \"value\": r2}\n",
    "    })\n",
    "# Print the timeline (for testing)\n",
    "for i, trial in enumerate(timeline):\n",
    "    print(f\"Trial {i+1}: {trial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_1 = TimelineVariable(\"bandit_1\")\n",
    "bandit_2 = TimelineVariable(\"bandit_2\")\n",
    "\n",
    "score = SharedVariable(\"score\", 0)\n",
    "value = DataVariable(\"value\", 0)\n",
    "\n",
    "update_score = FunctionVariable(\n",
    "    \"update_score\", lambda sc, val: sc + val, [score, value]\n",
    ")\n",
    "\n",
    "update_score_side_effect = SideEffect(score, update_score)\n",
    "\n",
    "bandit_task = Bandit(\n",
    "    bandits=[bandit_1, bandit_2],\n",
    "    side_effects=[update_score_side_effect],\n",
    ")\n",
    "\n",
    "score_text = FunctionVariable(\"score_text\", lambda sc: f\"Score: {sc}\", [score])\n",
    "\n",
    "show_score = Text(duration=1000, text=score_text)\n",
    "\n",
    "trial_sequence = Block([bandit_task, show_score], timeline=timeline)\n",
    "experiment = Experiment([instruction_block,trial_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.to_html(\"bandit_new.html\", path_local_download=\"2armed_bandit.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
